{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6dc67cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1ec5f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e60c62c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 5])\n"
     ]
    }
   ],
   "source": [
    "m = nn.Linear(3, 5)\n",
    "input = torch.randn(30, 3)\n",
    "output = m(input)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b074c41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.2815,  0.4312,  0.2928],\n",
      "        [ 0.4351, -0.4102,  0.5231],\n",
      "        [ 0.4547,  0.0486, -0.2896],\n",
      "        [ 0.2401,  0.3523,  0.1633],\n",
      "        [ 0.4645, -0.4825,  0.5104]], requires_grad=True)\n",
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "print(m.weight)\n",
    "print(m.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48412723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 2])\n"
     ]
    }
   ],
   "source": [
    "m = nn.Linear(3, 2)\n",
    "input = torch.randn(5, 3)\n",
    "output = m(input)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b93bc2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7604,  0.7423],\n",
      "        [ 1.1111,  0.3386],\n",
      "        [ 0.9002, -0.5376],\n",
      "        [ 0.5250, -0.3110],\n",
      "        [ 0.0945,  0.5233]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92978310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2168,  0.5998, -0.7084],\n",
      "        [ 1.3243,  0.3429,  0.4462],\n",
      "        [-0.1091, -0.4260,  1.4350],\n",
      "        [-0.4175, -0.4694,  0.2484],\n",
      "        [-0.6214,  1.3768,  0.5092]])\n"
     ]
    }
   ],
   "source": [
    "print(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a6bfa35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.4136, -0.2181,  0.2167],\n",
      "        [ 0.1657,  0.5059, -0.2525]], requires_grad=True)\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "print(m.weight)\n",
    "print(m.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96f32760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([0.5415, 0.0583], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(m.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0199b8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 2])\n"
     ]
    }
   ],
   "source": [
    "#different Dimension 3D vs 2d weight\n",
    "m = nn.Linear(3, 2)\n",
    "input = torch.randn(2, 5, 3)\n",
    "output = m(input)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6bb23c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4020,  0.9819, -0.6262],\n",
      "        [ 1.0931,  0.1125,  1.3712],\n",
      "        [-1.0081,  2.2841,  0.8125],\n",
      "        [ 0.1829,  1.9974,  0.6805],\n",
      "        [-0.9521, -0.3974, -1.5116]])\n",
      "Parameter containing:\n",
      "tensor([[-0.3362, -0.5557,  0.1012],\n",
      "        [ 0.0342, -0.1919, -0.5346]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.5310,  0.4198], requires_grad=True)\n",
      "tensor([[-1.0048,  0.5524],\n",
      "        [-0.8223, -0.2975],\n",
      "        [-1.3791, -0.4873],\n",
      "        [-1.6336, -0.3210],\n",
      "        [-0.1431,  1.2716]], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(input[0])\n",
    "print(m.weight)\n",
    "print(m.bias)\n",
    "print(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "78daa72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.1956,  0.3942, -0.1056],\n",
      "        [ 2.4017, -1.2683, -1.5703],\n",
      "        [ 0.0201,  0.9788,  1.0141],\n",
      "        [-1.2786,  0.6511,  0.2436],\n",
      "        [ 0.2166, -2.0369, -0.4048]])\n",
      "Parameter containing:\n",
      "tensor([[-0.3362, -0.5557,  0.1012],\n",
      "        [ 0.0342, -0.1919, -0.5346]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.5310,  0.4198], requires_grad=True)\n",
      "tensor([[ 0.3136,  0.2914],\n",
      "        [-0.7926,  1.5847],\n",
      "        [-0.9790, -0.3095],\n",
      "        [-0.4383,  0.1209],\n",
      "        [ 0.4871,  1.0345]], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(input[1])\n",
    "print(m.weight)\n",
    "print(m.bias)\n",
    "print(output[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "03164b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding diff size tensor\n",
    "input_a = torch.randn(8, 512, 64)\n",
    "input_b = torch.randn(1, 512, 64)\n",
    "\n",
    "input_c = input_a + input_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "024162a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 512, 64])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c9337846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0551,  0.7041,  1.5120],\n",
      "        [-0.4691, -0.2462, -0.0187],\n",
      "        [ 0.7619, -0.5605, -0.2905],\n",
      "        [ 0.7746,  0.8708,  0.7130],\n",
      "        [ 0.3988,  0.5639,  0.4560]])\n",
      "tensor([[ 1.9520, -0.3821, -0.5042]])\n",
      "tensor([[ 3.0070,  0.3219,  1.0078],\n",
      "        [ 1.4829, -0.6284, -0.5229],\n",
      "        [ 2.7138, -0.9427, -0.7948],\n",
      "        [ 2.7265,  0.4887,  0.2088],\n",
      "        [ 2.3508,  0.1818, -0.0482]])\n"
     ]
    }
   ],
   "source": [
    "input_a = torch.randn(5, 3)\n",
    "input_b = torch.randn(1, 3)\n",
    "input_c = input_a + input_b\n",
    "print(input_a)\n",
    "print(input_b)\n",
    "print(input_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "69c3a2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#softmax attention 2D\n",
    "input_attn = torch.randn(5, 3)\n",
    "output_softmax = F.softmax(input_attn, dim= -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "4516b550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4420, -1.3317, -0.3135],\n",
       "        [ 1.6815, -1.8824,  0.3285],\n",
       "        [-1.0429,  2.1300, -0.8862],\n",
       "        [ 0.0844,  0.0209, -0.6018],\n",
       "        [-0.3678, -0.8172,  1.1517]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "981b04c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3925, 0.1612, 0.4463],\n",
       "        [0.7771, 0.0220, 0.2009],\n",
       "        [0.0384, 0.9167, 0.0449],\n",
       "        [0.4095, 0.3843, 0.2062],\n",
       "        [0.1611, 0.1028, 0.7361]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7ba991ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#softmax attention 3D\n",
    "input_attn = torch.randn(2,5, 3)\n",
    "output_softmax = F.softmax(input_attn, dim= -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8e65fd6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4619,  0.3188,  2.3744],\n",
       "        [ 0.3249, -0.6101, -1.1520],\n",
       "        [-0.2039, -0.7993, -2.5554],\n",
       "        [-1.8620, -1.0116,  0.0179],\n",
       "        [ 0.2838, -1.0790, -0.7506]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_attn[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ad49500a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1158, 0.1004, 0.7839],\n",
       "        [0.6169, 0.2422, 0.1409],\n",
       "        [0.6073, 0.3348, 0.0578],\n",
       "        [0.1011, 0.2366, 0.6623],\n",
       "        [0.6206, 0.1588, 0.2206]])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_softmax[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "7140bb65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.6664,  1.1483,  0.8893],\n",
       "        [ 0.6653, -1.2694, -0.0069],\n",
       "        [-0.4746, -1.0764,  0.9368],\n",
       "        [-0.3177, -0.4273, -0.2695],\n",
       "        [ 1.1880,  0.8569, -0.4847]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_attn[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "34ce84e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4865, 0.2898, 0.2237],\n",
       "        [0.6042, 0.0873, 0.3085],\n",
       "        [0.1770, 0.0970, 0.7260],\n",
       "        [0.3395, 0.3043, 0.3563],\n",
       "        [0.5247, 0.3768, 0.0985]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_softmax[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a548d2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,d_k,d_model,n_heads):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Assume d_v = d_k\n",
    "        self.d_k = d_k\n",
    "        self.n_heads = n_heads\n",
    "        \n",
    "        self.key = nn.Linear(d_model,d_k * n_heads)\n",
    "        self.query = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.value = nn.Linear(d_model, d_k * n_heads)\n",
    "        \n",
    "        #final linear layer\n",
    "        self.fc = nn.Linear(d_k * n_heads, d_model)\n",
    "        \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "\n",
    "        #Note it's same for q,k,v dimension is same        \n",
    "        #q before  and self.query : torch.Size([8, 512, 64]) torch.Size([64, 64])\n",
    "        #q after : torch.Size([8, 512, 64])  \n",
    "        #it's like  operate 8 time of different each [512,64] * [64,64] = [512,64]\n",
    "           \n",
    "        q = self.query(q) #N x T x (hd_k)      \n",
    "        k = self.key(k) #N x T x (hd_k)\n",
    "        v = self.value(v) #N x T x (hd_k)\n",
    "\n",
    "        \n",
    "        N = q.shape[0] #N=8\n",
    "        T = q.shape[1] #T=512\n",
    "        \n",
    "        #change the shape to\n",
    "        #(N,T,h,d_k) -> (N,h,T,d_k)\n",
    "        #in order for matrix multiply works\n",
    "        \n",
    "        #n_heads = 4\n",
    "        #self.d_k = 16\n",
    "        #(N, T, self.n_heads, self.d_k) => (8,512,4,16)\n",
    "        \n",
    "        \n",
    "        q = q.view(N, T, self.n_heads, self.d_k).transpose(1,2)\n",
    "        #q,k,v after : [8, 4, 512, 16])  N,h,T,d_K\n",
    "        \n",
    "        k = k.view(N, T, self.n_heads, self.d_k).transpose(1,2)\n",
    "        v = v.view(N, T, self.n_heads, self.d_k).transpose(1,2)\n",
    "        \n",
    "        # compute attention weights\n",
    "        # (N,h,T,d_K) x (N,h,d_k,T) --> (N,h,T,T)\n",
    "        \n",
    "\n",
    "        #q [8, 4, 512, 16])  N,h,T,d_K\n",
    "        #k.transpose(-2,-1) :',torch.Size([8, 4, 16, 512])\n",
    "        #attn_scores [8, 4, 512, 512]\n",
    "        attn_scores = q @ k.transpose(-2,-1) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "           attn_scores = attn_scores.masked_fill(mask[:, None, None, :] == 0, float('-inf'))\n",
    "        \n",
    "        #attn_weights [8, 4, 512, 512] same as attn_scores N,h,T,T\n",
    "        attn_weights = F.softmax(attn_scores, dim= -1)\n",
    "        \n",
    "        \n",
    "        #compute attention-weighted values\n",
    "        #(N,h,T,T) * (N,h,T,d_k) -> (N,h,T,d_k)\n",
    "        \n",
    "        A = attn_weights @ v\n",
    "        \n",
    "        #reshape it back before final linear layer\n",
    "        A = A.transpose(1,2) #(N,T,h,d_k)\n",
    "        A = A.contiguous().view(N, T, self.d_k * self.n_heads) # (N,T,h*d_k)\n",
    "        \n",
    "        #A before is (8,512,4,16) and rearrange to A :[8, 512, 64]\n",
    "        #projection\n",
    "        return self.fc(A)\n",
    "        \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1e683742",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self,d_k, d_model, n_heads, dropout_prob=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.mha = MultiHeadAttention(d_k,d_model,n_heads)\n",
    "        self.ann = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model *4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model * 4, d_model),\n",
    "            nn.Dropout(dropout_prob),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        \n",
    "    def forward(self,x, mask=None):\n",
    "        #x [8, 512, 64], self.mha(x,x,x, mask) #attention block [8, 512, 64]\n",
    "        \n",
    "        x = self.ln1(x + self.mha(x,x,x, mask))\n",
    "        x = self.ln2(x + self.ann(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "17f0375a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,d_model, max_len=2048, dropout_prob=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        \n",
    "        position = torch.arange(max_len).unsqueeze(1)       #position : torch.Size([1024, 1])\n",
    "        exp_term = torch.arange(0,d_model,2)                #exp_term : torch.Size([32])\n",
    "        div_term = torch.exp(exp_term * (-math.log(10000.0) / d_model))     #div_term : torch.Size([32]) \n",
    "        pe = torch.zeros(1, max_len, d_model)   #pe : torch.Size([1, 1024, 64])\n",
    "        \n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe',pe)\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        # x.shape: N x T x D #torch.Size([8, 512, 64])\n",
    "        #self.pe[:,:x.size(1), :]  torch.Size([1, 512, 64])\n",
    "        #every row of data, has the same \"additional\" positional encoding\n",
    "        x = x + self.pe[:,:x.size(1), :]\n",
    "        #x after , still has size torch.Size([8, 512, 64])\n",
    "        \n",
    "        return self.dropout(x)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f40579de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size, #20_000\n",
    "                 max_len,    #1024\n",
    "                 d_k,        #16\n",
    "                 d_model,    #64\n",
    "                 n_heads,    #4\n",
    "                 n_layers,   #2\n",
    "                 n_classes,  #5\n",
    "                 dropout_prob): #0.1\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size,d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len, dropout_prob)\n",
    "        transformer_blocks = [\n",
    "            TransformerBlock(\n",
    "                d_k,\n",
    "                d_model,\n",
    "                n_heads,\n",
    "                dropout_prob) for _ in range(n_layers)]\n",
    "        self.transformer_blocks = nn.Sequential(*transformer_blocks)\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "        self.fc = nn.Linear(d_model, n_classes)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, mask= None):\n",
    "        \n",
    "        #x shape  torch.Size([8, 512])\n",
    "        \n",
    "        x = self.embedding(x)  #x shape after embed : torch.Size([8, 512, 64])\n",
    "        x = self.pos_encoding(x)  #x shape after pos encode: torch.Size([8, 512, 64])\n",
    "\n",
    "        #this loop 2 time, as in n_layers and each has same dimension\n",
    "        #x tranf block iterate  torch.Size([8, 512, 64])\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x,mask)\n",
    "\n",
    "        \n",
    "        #many to one (x has the shape N x T x D)\n",
    "        x = x[:,0,:]\n",
    "        x = self.ln(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044dad8d",
   "metadata": {},
   "source": [
    "model = Encoder(20_000,1024,16,64,4,2,5,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e896cbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Encoder(20_000,1024,16,64,4,2,5,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4af03815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (embedding): Embedding(20000, 64)\n",
       "  (pos_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha): MultiHeadAttention(\n",
       "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (fc): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (ann): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha): MultiHeadAttention(\n",
       "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (fc): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (ann): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  (fc): Linear(in_features=64, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d30ff376",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randint(0,20_000,size=(8,512))\n",
    "x_t = torch.tensor(x).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b1c483ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.ones((8,512))\n",
    "mask[:,256:] = 0\n",
    "mask_t = torch.tensor(mask).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a70a62f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xshape: torch.Size([8, 512, 64])\n",
      "self.pe[:,:x.size(1), :]: torch.Size([1, 512, 64])\n",
      "after xshape  torch.Size([8, 512, 64])\n"
     ]
    }
   ],
   "source": [
    "y = model(x_t, mask_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5226cdfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5878,  0.5137,  0.3711, -0.1257,  0.4204],\n",
       "        [ 1.3933, -1.0663, -0.0730, -0.1003, -0.1185],\n",
       "        [ 0.3367,  0.1441,  0.2411, -0.8343,  0.8958],\n",
       "        [-0.3690,  0.2260, -0.5430,  0.2643, -0.6687],\n",
       "        [-0.4171,  0.4315, -0.0418,  0.7934,  0.0919],\n",
       "        [ 0.5028,  0.5685, -0.0112,  0.3434,  0.9544],\n",
       "        [ 0.5288,  0.5362, -0.0085,  1.0734, -0.1748],\n",
       "        [ 0.2549,  1.0098, -0.0616, -0.6088,  0.7905]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "82232185",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### LET\"S DEBUG ONE BY ONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a2c8ebd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention_DEBUG_1(nn.Module):\n",
    "    def __init__(self,d_k,d_model,n_heads):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Assume d_v = d_k\n",
    "        self.d_k = d_k\n",
    "        self.n_heads = n_heads\n",
    "        \n",
    "        self.key = nn.Linear(d_model,d_k * n_heads)\n",
    "        self.query = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.value = nn.Linear(d_model, d_k * n_heads)\n",
    "        \n",
    "        #final linear layer\n",
    "        self.fc = nn.Linear(d_k * n_heads, d_model)\n",
    "        \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        q = self.query(q) #N x T x (hd_k)\n",
    "        k = self.key(k) #N x T x (hd_k)\n",
    "        v = self.value(v) #N x T x (hd_k)\n",
    "        \n",
    "        N = q.shape[0]\n",
    "        T = q.shape[1]\n",
    "        \n",
    "        #change the shape to\n",
    "        #(N,T,h,d_k) -> (N,h,T,d_k)\n",
    "        #in order for matrix multiply works\n",
    "        \n",
    "        q = q.view(N, T, self.n_heads, self.d_k).transpose(1,2)\n",
    "        k = k.view(N, T, self.n_heads, self.d_k).transpose(1,2)\n",
    "        v = v.view(N, T, self.n_heads, self.d_k).transpose(1,2)\n",
    "        \n",
    "        # compute attention weights\n",
    "        # (N,h,T,d_K) x (N,h,d_k,T) --> (N,h,T,T)\n",
    "        \n",
    "        attn_scores = q @ k.transpose(-2,-1) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "           attn_scores = attn_scores.masked_fill(mask[:, None, None, :] == 0, float('-inf'))\n",
    "        \n",
    "        attn_weights = F.softmax(attn_scores, dim= -1)\n",
    "        \n",
    "        #compute attention-weighted values\n",
    "        #(N,h,T,T) * (N,h,T,d_k) -> (N,h,T,d_k)\n",
    "        \n",
    "        A = attn_weights @ v\n",
    "        \n",
    "        #reshape it back before final linear layer\n",
    "        A = A.transpose(1,2) #(N,T,h,d_k)\n",
    "        A = A.contiguous().view(N, T, self.d_k * self.n_heads) # (N,T,h*d_k)\n",
    "        \n",
    "        #projection\n",
    "        return self.fc(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7266a64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultiHeadAttention_DEBUG_1(\n",
       "  (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (fc): Linear(in_features=64, out_features=64, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_debug1 = MultiHeadAttention_DEBUG_1(16,64,4)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model_debug1.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "25dc6078",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 must have the same dtype",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [55]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m20_000\u001b[39m,size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m512\u001b[39m))\n\u001b[0;32m      2\u001b[0m x_t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(x)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m----> 3\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_debug1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\mike_enva\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [50]\u001b[0m, in \u001b[0;36mMultiHeadAttention_DEBUG_1.forward\u001b[1;34m(self, q, k, v, mask)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, q, k, v, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 17\u001b[0m     q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#N x T x (hd_k)\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey(k) \u001b[38;5;66;03m#N x T x (hd_k)\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue(v) \u001b[38;5;66;03m#N x T x (hd_k)\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\mike_enva\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\mike_enva\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 must have the same dtype"
     ]
    }
   ],
   "source": [
    "x = np.random.randint(0,20_000,size=(8,512))\n",
    "x_t = torch.tensor(x).to(device)\n",
    "y = model_debug1(x_t,x_t,x_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a04a6b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = nn.Linear(10, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "083fb887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 10])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46226c7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
